# -*- coding: utf-8 -*-
"""assessing_structures_YOLOv7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ztcELQ34c5EDsLivUE5rRGozpdIAIhiy

# Building Detection from High-Resolution Satellite Images Using YOLOv7: Assessing Collapsed and Non-Collapsed Structures

<br>

<p align="center">
    <img src="https://drive.google.com/uc?id=1dx98iliCTjuonFHX_orRz20O8S7CeTnW" height=300 width=2000 alt="GitHub">
</p>

<small>Picture Source: <a href="https://github.com/doguilmak">Doğu İlmak</a></small>

<br>

## Introduction
In recent years, the field of computer vision has witnessed remarkable advancements, empowering various applications ranging from autonomous driving to surveillance systems. One critical task within computer vision is the detection and localization of objects within images. This capability finds particular relevance in disaster management, urban planning, and infrastructure assessment, where the accurate identification of buildings from satellite imagery plays a crucial role. In this scientific project, we present an approach for collapsed and non-collapsed building detection using YOLOv7, a state-of-the-art object detection algorithm, applied to high-resolution satellite images obtained from Maxar. By harnessing the power of deep learning and leveraging the spatial details captured by these satellite images, we aim to contribute to the field of disaster response and urban infrastructure analysis.

<br>

The dataset utilized in this project consists of 900 high-resolution satellite images, which were carefully selected to represent diverse urban environments. Out of these, 334 distinct images were used for training the YOLOv7 model, while 124 images were reserved for validation purposes. The training images were specifically chosen to encompass a wide range of building sizes, orientations, and structural complexities, ensuring the robustness and generalization of the model.

<br>

The YOLOv7 algorithm, short for "You Only Look Once version 7," is renowned for its real-time object detection capabilities and high accuracy. It utilizes a deep neural network architecture to simultaneously predict bounding boxes and class probabilities for multiple objects within an image. This holistic approach not only ensures rapid inference times but also provides a comprehensive understanding of the spatial distribution of buildings in the satellite images. To facilitate training and evaluation, all images were resized to a resolution of 640x640 pixels. The YOLOv7 model was then trained using a combination of advanced optimization techniques, including stochastic gradient descent and backpropagation, to iteratively improve its ability to detect collapsed and non-collapsed buildings accurately. Throughout the training process, the model learns to recognize distinctive features and patterns associated with buildings, enabling it to make accurate predictions on unseen data.

<br>

The validation dataset consisting of 124 images served as a benchmark for evaluating the performance of our trained model. By comparing the predicted bounding boxes with ground truth annotations, we measured metrics such as precision, recall, and F1 score, quantifying the model's ability to correctly detect buildings and distinguish between collapsed and non-collapsed structures. Through this scientific project, we aim to contribute to the ongoing efforts in disaster management and urban planning by providing a robust and efficient method for building detection from high-resolution satellite images. The accurate identification of collapsed structures can significantly aid search and rescue operations in disaster-stricken areas, while the detection of intact buildings can assist in assessing urban development and infrastructure planning.

<br>

In the subsequent sections, we will delve into the methodology employed, discuss the experimental setup, present the results obtained, and analyze the performance of the YOLOv7 model in detecting collapsed and non-collapsed buildings. This research represents a stepping stone towards the development of automated systems for rapid assessment and response during critical situations, ultimately contributing to the resilience and safety of our cities and communities.

<br>

## YOLO


*   **Single Pass Detection**: YOLO takes a different approach compared to traditional object detection methods that use region proposal techniques. Instead of dividing the image into regions and examining each region separately, YOLO performs detection in a single pass. It divides the input image into a grid and predicts bounding boxes and class probabilities for each grid cell.


*   **Grid-based Prediction**: YOLO divides the input image into a fixed-size grid, typically, say, 7x7 or 13x13. Each grid cell is responsible for predicting objects that fall within it. For each grid cell, YOLO predicts multiple bounding boxes (each associated with a confidence score) and class probabilities.

*   **Anchor Boxes**: To handle objects of different sizes and aspect ratios, YOLO uses anchor boxes. These anchor boxes are pre-defined boxes of different shapes and sizes. Each anchor box is associated with a specific grid cell. The network predicts offsets and dimensions for anchor boxes relative to the grid cell, along with the confidence scores and class probabilities.

*   **Training**: YOLO is trained using a combination of labeled bounding box annotations and classification labels. The training process involves optimizing the network to minimize the localization loss (related to the accuracy of bounding box predictions) and the classification loss (related to the accuracy of class predictions).

*   **Speed and Accuracy Trade-off**: YOLO achieves real-time object detection by sacrificing some localization accuracy compared to slower methods like Faster R-CNN. However, it still achieves competitive accuracy while providing significantly faster inference speeds, making it well-suited for real-time applications.

<br>

Since its introduction, YOLO has undergone several improvements and variations. Different versions such as YOLOv2, YOLOv3, and YOLOv4 have been developed, each introducing enhancements in terms of accuracy, speed, and additional features.

It's important to note that this is a high-level overview of YOLO, and the algorithm has many technical details and variations. For a more in-depth understanding, it's recommended to refer to the original YOLO papers and related resources.

<br>

## Keywords

*   Building detection
*   YOLOv7
*   Object detection
*   High-resolution satellite images
*   Disaster management
*   Urban planning
*   Deep learning
*   Object detection
*   Collapsed structures

<br>

Make sure your runtime is **GPU** (_not_ CPU or TPU). And if it is an option, make sure you are using _Python 3_. You can select these settings by going to `Runtime -> Change runtime type -> Select the above mentioned settings and then press SAVE`.

## Importing Libraries
"""

from google.colab import files
from google.colab import drive

import os
import random
import shutil

import moviepy.editor as mp
from IPython.display import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.image as mpimg
from PIL import Image

import warnings
warnings.filterwarnings("ignore")

"""## Data Preprocessing"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/

from google.colab import files
uploaded = files.upload()

!unzip -q /content/data.zip

# !rm -rf /content/archive.zip

#@markdown ---
#@markdown ### Enter image paths:
train_images_dir = "/content/train/images" #@param {type:"string"}
val_images_dir = "/content/val/images" #@param {type:"string"}

train_image_count = len([f for f in os.listdir(train_images_dir) if f.endswith(".png")])
val_image_count = len([f for f in os.listdir(val_images_dir) if f.endswith(".png")])

print(f"Number of images in train folder: {train_image_count}")
print(f"Number of images in val folder: {val_image_count}")

#@markdown ---
#@markdown ### Enter label paths:
train_labels_dir = "/content/train/labels" #@param {type:"string"}
val_labels_dir = "/content/val/labels" #@param {type:"string"}

train_txt_count = len([f for f in os.listdir(train_labels_dir) if f.endswith(".txt")])
val_txt_count = len([f for f in os.listdir(val_labels_dir) if f.endswith(".txt")])

print(f"Number of TXT files in train labels folder: {train_txt_count}")
print(f"Number of TXT files in val labels folder: {val_txt_count}")

#@markdown You can plot any image and labels you want.

#@markdown `classes = {0: collapsed, 1: non_collapsed}`

image_path = 'train/images/image_17.png' #@param {type:"string"}
label_path = 'train/labels/image_17.txt' #@param {type:"string"}

image = Image.open(image_path)
image_width, image_height = image.size
fig, ax = plt.subplots(1)
ax.imshow(image)

unique_classes = {}

with open(label_path, 'r') as file:
    for line in file:

        class_label, x, y, width, height = line.split()

        x = float(x) * image_width
        y = float(y) * image_height
        width = float(width) * image_width
        height = float(height) * image_height

        x_min = x - width / 2
        y_min = y - height / 2

        if class_label not in unique_classes:
            unique_classes[class_label] = f"C{len(unique_classes) + 1}"

        box_color = unique_classes[class_label]

        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=1, edgecolor=box_color, facecolor='none')
        ax.add_patch(rect)


ax.set_aspect('equal')
legend_handles = [patches.Patch(color=color, label=f"Class {label}") for label, color in unique_classes.items()]
ax.legend(handles=legend_handles)
plt.show()

"""## Clone YOLO v7 and Train the Model"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/

"""Following URL of the GitHub repository that contains the YOLOv7 implementation by WongKinYiu. The .git extension indicates that it is a Git repository.

When this command is executed, it will create a local copy of the entire repository on your machine. This allows you to access and use the YOLOv7 implementation provided by WongKinYiu for your own projects. You can then navigate to the cloned repository and access the relevant files and directories, including the training script, configuration files, and pre-trained weights.
"""

!git clone https://github.com/WongKinYiu/yolov7.git

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/yolov7

"""You can change model from [WongKinYiu GitHub](https://github.com/WongKinYiu/yolov7) page. When this command is executed, it downloads the "yolov7.pt" file from the specified URL. This pre-trained weights file can then be used as a starting point for training or as a pre-trained model for inference in YOLOv7-based projects.

Regarding other ".pt" files for YOLO models, there are different variations of the YOLO architecture, each with its own pre-trained weights. Some commonly used pre-trained weights files for YOLO models include:

* `yolov3.pt`: Pre-trained weights for YOLOv3.
* `yolov4.pt`: Pre-trained weights for YOLOv4.
* `yolov5s.pt`, yolov5m.pt, yolov5l.pt, yolov5x.pt: Pre-trained weights for YOLOv5, which is a different version developed by Ultralytics.

These pre-trained weights files can be obtained from various sources, including official repositories, GitHub releases, or community contributions. It's important to note that the architecture and compatibility of the weights file should match the specific YOLO version you are using in your project to ensure proper functionality.
"""

!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt

"""Before training, you neet to go to `yolov7/data/coco.yaml` and define your number of class, class names and train-val paths like that:

```
# COCO 2017 dataset http://cocodataset.org

train: ../train
val: ../val

# number of classes
nc: 2

# class names
names: ['collapsed', 'non_collapsed']
```

*   `!python train.py`: This is the command to execute the Python script *train.py* for training the YOLOv7 model.

*  `--device 0`: This parameter specifies the device (GPU) to be used for training. In this case, it is set to device 0, indicating the first GPU device.

*  `--batch-size 8`: This parameter determines the number of images in each batch during training. A batch size of *8* means that the model will process 8 images at a time before updating the weights.

*  `--data data/coco.yaml`: This parameter specifies the path to the YAML file containing the dataset configuration. In this case, **the *coco.yaml* file is used, which provides information about the dataset, including the classes and paths to the training and validation data.**

*  `--img 640 640`: This parameter sets the input image size for the model. The YOLOv7 model requires square input images, and here the dimensions are set to 640x640 pixels.

*  `--epochs 64`: This parameter defines the number of epochs, which represents the number of times the entire training dataset will be passed through the model during training. In this case, the model will be trained for **64 epochs**.

*  `--weights yolov7.pt`: This parameter specifies the initial weights of the model. The *yolov7.pt* file contains the pre-trained weights for the *YOLOv7* model, which will be used as the starting point for training.

*  `--hyp data/hyp.scratch.p5.yaml`: This parameter indicates the path to the YAML file containing hyperparameters for training. Hyperparameters include learning rate, weight decay, and other settings that affect the training process. Here, the *hyp.scratch.p5.yaml* file is used.

*  `--name yolov7`: This parameter sets the name of the model during training. The name can be customized, and in this case, it is set to *yolov7*.

If you are using GPU, try this:
"""

!python train.py --device 0 --batch-size 8 --data data/coco.yaml --img 640 640 --epochs 64 --weights yolov7.pt --hyp data/hyp.scratch.p5.yaml --name yolov7

"""If you are using only CPU, try this:"""

# !python train.py --device cpu --batch-size 16 --data data/coco.yaml --img 640 640 --epochs 64 --weights yolov7.pt --hyp data/hyp.scratch.p5.yaml --name yolov7x

image = mpimg.imread('/content/yolov7/runs/train/yolov7/results.png')
plt.figure(figsize=(16, 9), dpi=300)
plt.title('Results')
plt.imshow(image)
plt.axis('off')
plt.show()

image1 = mpimg.imread('/content/yolov7/runs/train/yolov7/test_batch0_labels.jpg')
image2 = mpimg.imread('/content/yolov7/runs/train/yolov7/test_batch0_pred.jpg')

fig, axes = plt.subplots(1, 2, figsize=(16, 9), dpi=300)
axes[0].imshow(image1)
axes[0].set_title('Labels')
axes[0].axis('off')
axes[1].imshow(image2)
axes[1].set_title('Predictions')
axes[1].axis('off')
plt.show()

"""## Uplad Test Image and Video to Make Prediction

Video Testing:

*   Select a video file that contains satellite image footage.
*   Load the pre-trained YOLOv7 model and its corresponding weights.
*   Utilize the OpenCV library to read and process each frame of the video.
*   Pass each frame through the drone detection model for real-time inference.
*   Draw bounding boxes around detected buildings and display the annotated video output.


Image Testing:

*   Choose an image that includes a building satellite image.
*   Load the pre-trained YOLOv7 model and its weights.
*   Read and process the image.
*   Apply the trained model to the image and identify the presence of a buildings.
*   Visualize the image with a bounding box around the detected buildings, if present.
*   Analyze the model's performance by assessing the correct identification of the buildings.
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/
!mkdir test
# %cd /content/test

uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/yolov7

"""Upload your images into test folder."""

!python detect.py --weights runs/train/yolov7/weights/best.pt --conf 0.50 --img-size 640 --source /content/test/pexels-petar-avramoski-6081353.jpg

"""In addition, you can upload video and make predictions."""

!python detect.py --weights runs/train/yolov7/weights/best.pt --conf 0.25 --img-size 640 --source /content/test/pexels-nino-souza-8459631-1920x1080-30fps.mp4

#@markdown ### Enter video path:
video_path = '' #@param {type:"string"} # Replace with the actual path to your video file
#@markdown ---
#@markdown ### Enter gif path:
gif_path = '' #@param {type:"string"} # Replace with the desired path for the output GIF file

clip = mp.VideoFileClip(video_path)
clip.write_gif(gif_path)

Image(gif_path)

"""## Save the Results"""

files.download('/content/yolov7/runs/train/yolov7/train_batch0.jpg')
files.download('/content/yolov7/runs/train/yolov7/train_batch1.jpg')
files.download('/content/yolov7/runs/train/yolov7/train_batch2.jpg')
files.download('/content/yolov7/runs/train/yolov7/train_batch3.jpg')
files.download('/content/yolov7/runs/train/yolov7/train_batch4.jpg')
files.download('/content/yolov7/runs/train/yolov7/train_batch5.jpg')
files.download('/content/yolov7/runs/train/yolov7/train_batch6.jpg')
files.download('/content/yolov7/runs/train/yolov7/train_batch7.jpg')
files.download('/content/yolov7/runs/train/yolov7/train_batch8.jpg')
files.download('/content/yolov7/runs/train/yolov7/train_batch9.jpg')

files.download('/content/yolov7/runs/train/yolov7/test_batch0_labels.jpg')
files.download('/content/yolov7/runs/train/yolov7/test_batch0_pred.jpg')
files.download('/content/yolov7/runs/train/yolov7/test_batch1_labels.jpg')
files.download('/content/yolov7/runs/train/yolov7/test_batch1_pred.jpg')
files.download('/content/yolov7/runs/train/yolov7/test_batch2_labels.jpg')
files.download('/content/yolov7/runs/train/yolov7/test_batch2_pred.jpg')

files.download('/content/yolov7/runs/train/yolov7/F1_curve.png')
files.download('/content/yolov7/runs/train/yolov7/PR_curve.png')
files.download('/content/yolov7/runs/train/yolov7/confusion_matrix.png')
files.download('/content/yolov7/runs/train/yolov7/hyp.yaml')
files.download('/content/yolov7/runs/train/yolov7/opt.yaml')
files.download('/content/yolov7/runs/train/yolov7/results.png')
files.download('/content/yolov7/runs/train/yolov7/results.txt')

# files.download('runs/detect/exp/')
files.download('/content/yolov7/data/coco.yaml')
files.download('runs/train/yolov7/weights/best.pt')

"""## Conclusion
In this scientific project, we presented an approach for detecting collapsed and non-collapsed buildings from high-resolution satellite images using the *YOLOv7* object detection algorithm. By leveraging the power of deep learning and utilizing Maxar's high-resolution satellite imagery, we aimed to contribute to disaster response and urban infrastructure analysis. Through careful selection and preprocessing of a dataset comprising 900 satellite images, we trained the *YOLOv7* model using 334 images while reserving 124 images for validation. The model was trained to recognize distinctive features and patterns associated with buildings, enabling it to accurately predict bounding boxes and class probabilities.

<br>

The evaluation of our trained model on the validation dataset revealed promising results. By comparing predicted bounding boxes with ground truth annotations, we measured metrics such as *precision*, *recall*, and *F1 score*, demonstrating the model's ability to detect buildings and differentiate between **collapsed and non-collapsed structures**. Our research represents a significant step towards automated systems for rapid assessment and response during critical situations. The accurate identification of collapsed structures can greatly aid search and rescue operations, while the detection of intact buildings can assist in urban development and infrastructure planning.

<br>

However, there are still areas for improvement and further exploration. The model's performance can be enhanced by incorporating more diverse and extensive datasets, including satellite images from various geographical locations and different disaster scenarios. Additionally, *fine-tuning* the model with additional training iterations and exploring advanced optimization techniques could potentially yield even better results. The application of *deep learning* algorithms for building detection from satellite imagery has enormous potential in various fields, including *disaster management*, *urban planning*, and *infrastructure assessment*. As technology continues to advance, we anticipate further improvements in accuracy, speed, and scalability, making these automated systems invaluable in supporting decision-making processes and enhancing overall safety and resilience.

<br>

In conclusion, our scientific project demonstrates the effectiveness of YOLOv7 in detecting collapsed and non-collapsed buildings from high-resolution satellite images. By bridging the gap between computer vision and remote sensing, we have made significant strides towards leveraging artificial intelligence to enhance disaster response and urban planning, ultimately contributing to the well-being of communities worldwide..

## Contact Me
<p>If you have something to say to me please contact me:</p>

<ul>
  <li>Twitter: <a href="https://twitter.com/Doguilmak">Doguilmak</a></li>
  <li>Mail address: doguilmak@gmail.com</li>
</ul>
"""

from datetime import datetime
print(f"Changes have been made to the project on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")